{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02a23fe-daeb-4422-8f0a-0f5403dc8ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /opt/conda/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (1.4.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from umap-learn) (4.66.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn) (0.42.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.11/site-packages (from pynndescent>=0.5->umap-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=0.22->umap-learn) (3.4.0)\n",
      "Requirement already satisfied: mplhep in /opt/conda/lib/python3.11/site-packages (0.3.55)\n",
      "Requirement already satisfied: matplotlib>=3.4 in /opt/conda/lib/python3.11/site-packages (from mplhep) (3.8.4)\n",
      "Requirement already satisfied: mplhep-data>=0.0.4 in /opt/conda/lib/python3.11/site-packages (from mplhep) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from mplhep) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from mplhep) (24.0)\n",
      "Requirement already satisfied: uhi>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from mplhep) (0.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.4->mplhep) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.4->mplhep) (1.16.0)\n",
      "Requirement already satisfied: psycopg2-binary in ./.local/lib/python3.11/site-packages (2.9.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 07:11:12.371445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: psycopg2-binary\n",
      "Version: 2.9.10\n",
      "Summary: psycopg2 - Python-PostgreSQL Database Adapter\n",
      "Home-page: https://psycopg.org/\n",
      "Author: Federico Di Gregorio\n",
      "Author-email: fog@initd.org\n",
      "License: LGPL with exceptions\n",
      "Location: /home/jovyan/.local/lib/python3.11/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#Necessary Imports\n",
    "import os\n",
    "#os.system(\"pip freeze > requirements.txt\")\n",
    "#os.system(\"pip install -r requirements.txt\")\n",
    "os.system(\"pip install umap-learn\")\n",
    "os.system(\"pip install mplhep\")\n",
    "os.system(\"pip install psycopg2-binary --user\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import umap\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import dask.array as da\n",
    "\n",
    "import tensorflow as tf\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "from umap.parametric_umap import ParametricUMAP as PUMAP\n",
    "import sys\n",
    "os.system(\"pip show psycopg2-binary\")\n",
    "sys.path.append(\"/home/jovyan/.local/lib/python3.11/site-packages\")\n",
    "\n",
    "import psycopg2\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4a294-b68d-400f-91c5-d5cf60731497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(host=\"adc-2021.c7skue2e0u6i.us-east-1.rds.amazonaws.com\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"UM#37tz;80\",\n",
    "                        port=\"5432\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set the schema to be used\n",
    "cur.execute(\"SET search_path TO l1anomaly\")\n",
    "random_background_indices = np.random.choice(2000000, size=2000000, replace=False)\n",
    "random_signal_indices = np.random.choice(690000, size=690000, replace=False)\n",
    "\n",
    "# Convert numpy arrays to comma-separated strings\n",
    "random_background_indices_str = ', '.join(map(str, random_background_indices))\n",
    "random_signal_indices_str = ', '.join(map(str, random_signal_indices))\n",
    "\n",
    "cur.execute(f\"SELECT * FROM background_for_training WHERE idbackground_for_training IN ({random_background_indices_str})\")\n",
    "background_data = cur.fetchall()\n",
    "\n",
    "cur.execute(f\"SELECT * FROM hToTauTau_13TeV_PU20_filtered WHERE idhToTauTau_13TeV_PU20_filtered IN ({random_signal_indices_str})\")\n",
    "signal_data = cur.fetchall()\n",
    "\n",
    "background_data = np.array(background_data)\n",
    "signal_data = np.array(signal_data)\n",
    "\n",
    "#Let's try getting the data in with batching\n",
    "\n",
    "# Function to query in batches and collect data\n",
    "# def fetch_data_in_batches(cursor, table_name, id_column, id_list, batch_size=10000):\n",
    "#     data = []\n",
    "#     for i in range(0, len(id_list), batch_size):\n",
    "#         # Extract the current batch of IDs\n",
    "#         batch = id_list[i:i + batch_size]\n",
    "#         batch_str = ', '.join(map(str, batch))\n",
    "        \n",
    "#         # Execute the query for the current batch\n",
    "#         query = f\"SELECT * FROM {table_name} WHERE {id_column} IN ({batch_str})\"\n",
    "#         print(\"Query: \" + str(i))\n",
    "#         cursor.execute(query)\n",
    "        \n",
    "#         # Fetch and extend the data list\n",
    "#         data.extend(cursor.fetchall())\n",
    "#     return np.array(data)\n",
    "\n",
    "# Connect to the database\n",
    "# conn = psycopg2.connect(host=\"adc-2021.c7skue2e0u6i.us-east-1.rds.amazonaws.com\",\n",
    "#                         user=\"postgres\",\n",
    "#                         password=\"UM#37tz;80\",\n",
    "#                         port=\"5432\")\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# Set the schema to be used\n",
    "# cur.execute(\"SET search_path TO l1anomaly\")\n",
    "\n",
    "# Generate random indices for background and signal\n",
    "# random_background_indices = np.random.choice(13000000, size=3000000, replace=False)\n",
    "# random_signal_indices = np.random.choice(690000, size=690000, replace=False)\n",
    "\n",
    "# Fetch background data in batches\n",
    "# background_data = fetch_data_in_batches(\n",
    "#     cur,\n",
    "#     table_name=\"background_for_training\",\n",
    "#     id_column=\"idbackground_for_training\",\n",
    "#     id_list=random_background_indices,\n",
    "#     batch_size=500000  # Adjust batch size as needed\n",
    "# )\n",
    "\n",
    "# # Fetch signal data in batches\n",
    "# signal_data = fetch_data_in_batches(\n",
    "#     cur,\n",
    "#     table_name=\"hToTauTau_13TeV_PU20_filtered\",\n",
    "#     id_column=\"idhToTauTau_13TeV_PU20_filtered\",\n",
    "#     id_list=random_signal_indices,\n",
    "#     batch_size=100000\n",
    "# )\n",
    "\n",
    "# Signal label\n",
    "signal_label = \"$h^{{0}} \\\\to \\\\tau\\\\tau$\"\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddc63e-bcc1-4ff8-9a78-88ea86608285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def mse_loss(inputs, outputs):\n",
    "    #Mean distance squared between input and output tensors\n",
    "    return tf.math.reduce_mean((inputs - outputs) ** 2, axis=-1)\n",
    "\n",
    "# def make_mse_per_sample(inputs, outputs):\n",
    "#     outputs = tf.cast(outputs, dtype=inputs.dtype)  # make same type\n",
    "\n",
    "#     inputs = tf.reshape(inputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "#     outputs = tf.reshape(outputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "\n",
    "#     # extract pt\n",
    "#     outputs_pt = outputs[:, :, 0]\n",
    "\n",
    "#     # trick with phi (rescaled tanh activation function)\n",
    "#     outputs_phi = phi_max * tf.math.tanh(outputs[:, :, 2])\n",
    "\n",
    "#     # trick with eta (rescaled tanh activation function)\n",
    "#     outputs_eta_met = outputs[:, 0:1, 1]\n",
    "#     outputs_eta_ele = ele_eta_max * tf.math.tanh(\n",
    "#         outputs[:, ele_off : ele_off + nele, 1]\n",
    "#     )\n",
    "#     outputs_eta_mu = mu_eta_max * tf.math.tanh(outputs[:, mu_off : mu_off + nmu, 1])\n",
    "#     outputs_eta_jet = jet_eta_max * tf.math.tanh(\n",
    "#         outputs[:, jet_off : jet_off + njet, 1]\n",
    "#     )\n",
    "#     outputs_eta = tf.concat(\n",
    "#         [outputs_eta_met, outputs_eta_ele, outputs_eta_mu, outputs_eta_jet], axis=1\n",
    "#     )\n",
    "\n",
    "#     # use both tricks\n",
    "#     outputs = tf.stack([outputs_pt, outputs_eta, outputs_phi], axis=-1)\n",
    "\n",
    "#     # mask zero features\n",
    "#     mask = tf.math.not_equal(inputs, 0)\n",
    "#     mask = tf.cast(mask, dtype=outputs.dtype)\n",
    "#     outputs = mask * outputs\n",
    "\n",
    "#     loss = mse_loss(\n",
    "#         tf.reshape(inputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "#         tf.reshape(outputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "#     )\n",
    "#     return loss\n",
    "\n",
    "def make_mse_per_sample(inputs, outputs):\n",
    "    outputs = tf.cast(outputs, dtype=inputs.dtype)  # make inputs and outputs same type\n",
    "\n",
    "    #1+4+4+10 = 19 with 3 features of pT, eta, phi which are transverse momentum, pseduorapidity, azimuthal angle\n",
    "    # as in Main AE paper\n",
    "    inputs = tf.reshape(inputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "    outputs = tf.reshape(outputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "\n",
    "    # extract pt\n",
    "    outputs_pt = outputs[:, :, 0]\n",
    "    \n",
    "    # extract class\n",
    "    outputs_class = outputs[:, :, 3]\n",
    "\n",
    "    # trick with phi (rescaled tanh activation function) - pi times tanh of azimuthal angle\n",
    "    outputs_phi = phi_max * tf.math.tanh(outputs[:, :, 2])\n",
    "\n",
    "    #Extracts missing transverse energy pseudorapidity outputs\n",
    "    outputs_eta_met = outputs[:, 0:1, 1]\n",
    "\n",
    "    # trick with eta (rescaled tanh activation function) - max electron pseudorapidity times tanh of pseudorapidity\n",
    "    outputs_eta_ele = ele_eta_max * tf.math.tanh(\n",
    "        outputs[:, ele_off : ele_off + nele, 1]\n",
    "    )\n",
    "\n",
    "    #Treatment of muon pseudorapidity analogous to that of electron\n",
    "    outputs_eta_mu = mu_eta_max * tf.math.tanh(outputs[:, mu_off : mu_off + nmu, 1])\n",
    "\n",
    "    #Treatment of jet pseudorapidity analogous to that of electrons and muons\n",
    "    outputs_eta_jet = jet_eta_max * tf.math.tanh(\n",
    "        outputs[:, jet_off : jet_off + njet, 1]\n",
    "    )\n",
    "\n",
    "    #Output psuedorapidity is triple with missing transverse eneergy, electron, muon, jet\n",
    "    outputs_eta = tf.concat(\n",
    "        [outputs_eta_met, outputs_eta_ele, outputs_eta_mu, outputs_eta_jet], axis=1\n",
    "    )\n",
    "\n",
    "    # use both tricks - stacks into standard triple - transvere momenta, pseudorapidity, azimuthal angle\n",
    "    outputs = tf.stack([outputs_pt, outputs_eta, outputs_phi, outputs_class], axis=-1)\n",
    "\n",
    "    # mask zero features - Zero Padding after output formation\n",
    "    mask = tf.math.not_equal(inputs, 0)\n",
    "    mask = tf.cast(mask, dtype=outputs.dtype)\n",
    "    outputs = mask * outputs\n",
    "\n",
    "    #Apply previously defined MSE_loss function 1 - corresponding to nmet\n",
    "    loss = mse_loss(\n",
    "        tf.reshape(inputs, [-1, (nmet + nele + nmu + njet) * nfeat]),\n",
    "        tf.reshape(outputs, [-1, (nmet + nele + nmu + njet) * nfeat]),\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def make_mse(inputs, outputs):\n",
    "    loss = make_mse_per_sample(inputs, outputs)\n",
    "\n",
    "    loss = tf.math.reduce_mean(loss, axis=0)  # average over batch\n",
    "    loss = tf.cast(loss, dtype=inputs.dtype)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def scale_pt(X, pt_scaler=None):\n",
    "    pt = X[:, 0::nfeat]\n",
    "    if pt_scaler is None:\n",
    "        pt_scaler = StandardScaler()\n",
    "        pt_scaled = pt_scaler.fit_transform(pt)\n",
    "    else:\n",
    "        pt_scaled = pt_scaler.transform(pt)\n",
    "    X_scaled = np.copy(X)\n",
    "    X_scaled[:, 0::nfeat] = np.multiply(pt_scaled, pt != 0)\n",
    "    return X_scaled, pt_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e535a7-b552-4acc-804a-3b35982bc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fundamental Constants\n",
    "\n",
    "nfeat = 4\n",
    "nmet = 1\n",
    "nele = 4\n",
    "nmu = 4\n",
    "njet = 10\n",
    "ele_off = 1\n",
    "mu_off = nmet + nele\n",
    "jet_off = nmet + nele + nmu\n",
    "phi_max = np.pi\n",
    "ele_eta_max = 3.0\n",
    "mu_eta_max = 2.1\n",
    "jet_eta_max = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade1629-1de5-4922-8aa2-1d1c19588583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with Gaussian\n",
    "# background_data_reduced, extraneous = train_test_split(background_data, test_size = 0.5)\n",
    "\n",
    "\n",
    "X_train_val, X_test = train_test_split(background_data.reshape(background_data.shape[0], -1), test_size=0.2, shuffle=True)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.2, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "signal_data = signal_data.reshape(signal_data.shape[0], -1)\n",
    "merged_data = np.concatenate([X_test, signal_data], axis=0)\n",
    "merged_labels = np.concatenate([np.zeros(X_test.shape[0]), np.ones(signal_data.shape[0])], axis=0)\n",
    "merged_data_trans = scaler.transform(merged_data)\n",
    "merged_loss = np.sum(merged_data_trans ** 2, axis=-1)\n",
    "fpr, tpr, thresholds = roc_curve(merged_labels, merged_loss)\n",
    "tpr_1em5 = find_nearest(fpr, 1e-5)\n",
    "\n",
    "fpr_gaussian = fpr\n",
    "tpr_gaussian = tpr\n",
    "tpr_1em5_gaussian = tpr_1em5\n",
    "auc_gaussian = auc\n",
    "plt.plot(fpr_gaussian, tpr_gaussian, label=f\"Gaussian, AUC={auc(fpr_gaussian, tpr_gaussian)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_gaussian[tpr_1em5_gaussian]*100:.3f}%\")\n",
    "plt.legend(title = str(signal_label) +\" baseline\")\n",
    "plt.plot([1e-6, 1], [1e-6, 1], 'k--')\n",
    "plt.plot([1e-5, 1e-5], [1e-6, 1], 'r-.')\n",
    "plt.xlim([1e-6, 1])\n",
    "plt.ylim([1e-6, 1])\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb2b4d-777b-4203-a41e-6902e13e229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue with PUMAP\n",
    "X_train_scaled, pt_scaler = scale_pt(X_train)\n",
    "\n",
    "X_test_scaled, _ = scale_pt(X_test, pt_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b627ed3-eed9-4357-9caa-5aa318c7353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_pumap(X_scaled,inv_transform_data):\n",
    "    #Extract loss from model predictions using make_mse_per_sample function analyzed above\n",
    "    return np.array(make_mse_per_sample(X_scaled, inv_transform_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b8689-4044-4300-a01b-2434f81ba377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_pca(X_scaled,inv_transform_data):\n",
    "    #Extract loss from model predictions using make_mse_per_sample function analyzed above\n",
    "    return np.array(make_mse_per_sample(X_scaled, inv_transform_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba5024-b505-4a69-8cbb-6013db7454fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask array from the full dataset\n",
    "# X_train_scaled_dask = da.from_array(X_train_scaled, chunks=(1_000_000, X_train_scaled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380deff-94d4-48e5-beac-de09bd9fb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = PUMAP(low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42415d3-0182-4a79-8ee4-d39510a5adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainEmbedding = model.fit_transform(X_train_scaled)\n",
    "# We fit the entire dataset at once (but itâ€™s done in chunks internally)\n",
    "# This will avoid multiple fit calls but still scale efficiently\n",
    "# trainEmbedding = model.fit_transform(X_train_scaled_dask.compute())\n",
    "# inv_transform_training_data = model.inverse_transform(trainEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7afc0-cbe0-40bc-b848-d5fb26f22623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform in batches (to avoid memory overload)\n",
    "# batch_size = 1_000_000  # Adjust based on available memory\n",
    "# num_batches = len(trainEmbedding) // batch_size + 1\n",
    "\n",
    "# inv_transform_training_data = []\n",
    "\n",
    "# for i in range(num_batches):\n",
    "#     start = i * batch_size\n",
    "#     end = min((i + 1) * batch_size, len(trainEmbedding))\n",
    "#     print(f\"Inverse transforming batch {i + 1}/{num_batches}\")\n",
    "#     inv_batch = model.inverse_transform(trainEmbedding[start:end])\n",
    "#     inv_transform_training_data.append(inv_batch)\n",
    "\n",
    "# # Combine batches into a single array\n",
    "# inv_transform_training_data = np.vstack(inv_transform_training_data)\n",
    "inv_transform_training_data = model.inverse_transform(trainEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af727a8-5c42-4a21-a4e8-9193760912b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train_scaled).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c793a-68c5-4c1e-94c2-484c6a2e21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_loss = get_loss_pumap(X_train_scaled[:, 1:], inv_transform_training_data[:, 1:])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "signal_data = signal_data.reshape(signal_data.shape[0], -1)\n",
    "signal_data_scaled, _ = scale_pt(signal_data, pt_scaler)\n",
    "merged_labels = np.concatenate(\n",
    "    [np.zeros(X_train.shape[0]), np.ones(signal_data.shape[0])], axis=0\n",
    ")\n",
    "\n",
    "print(\"Starting PUMAP signal embedding: \" + signal_label)\n",
    "inv_transform_signal_data = model.inverse_transform(model.transform(signal_data_scaled))\n",
    "\n",
    "signal_loss = get_loss_pumap(signal_data_scaled[:, 1:], inv_transform_signal_data[:, 1:])\n",
    "merged_loss = np.concatenate([background_loss, signal_loss], axis=0)\n",
    "\n",
    "print(\"Successful PUMAP signal embedding, inverse transform, and loss computations: \" + signal_label)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(merged_labels, merged_loss)\n",
    "tpr_1em5 = find_nearest(fpr, 1e-5)\n",
    "\n",
    "fpr_pumap = fpr\n",
    "tpr_pumap = tpr\n",
    "tpr_1em5_pumap = tpr_1em5\n",
    "auc_pumap = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa2e0d-5251-4c36-8a66-4e21d8cb910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components=10)\n",
    "\n",
    "trainEmbedding = model.fit_transform(X_train_scaled)\n",
    "inv_transform_training_data = model.inverse_transform(trainEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82335ba-b1dd-40b5-a2f1-f96374eb9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_loss = get_loss_pca(X_train_scaled[:, 1:], inv_transform_training_data[:, 1:])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "signal_data = signal_data.reshape(signal_data.shape[0], -1)\n",
    "signal_data_scaled, _ = scale_pt(signal_data, pt_scaler)\n",
    "merged_labels = np.concatenate(\n",
    "    [np.zeros(X_train.shape[0]), np.ones(signal_data.shape[0])], axis=0\n",
    ")\n",
    "\n",
    "print(\"Starting PCA signal embedding: \" + signal_label)\n",
    "inv_transform_signal_data = model.inverse_transform(model.transform(signal_data_scaled))\n",
    "\n",
    "signal_loss = get_loss_pca(signal_data_scaled[:, 1:], inv_transform_signal_data[:, 1:])\n",
    "merged_loss = np.concatenate([background_loss, signal_loss], axis=0)\n",
    "\n",
    "print(\"Successful PCA signal embedding, inverse transform, and loss computations: \" + signal_label)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(merged_labels, merged_loss)\n",
    "tpr_1em5 = find_nearest(fpr, 1e-5)\n",
    "\n",
    "fpr_pca = fpr\n",
    "tpr_pca = tpr\n",
    "tpr_1em5_pca = tpr_1em5\n",
    "auc_pca = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c7e53-09e8-4ad6-b104-72f2615d92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mse_per_sample_ae_class(inputs, outputs):\n",
    "        outputs = tf.cast(outputs, dtype=inputs.dtype)  # make inputs and outputs same type\n",
    "\n",
    "        #1+4+4+10 = 19 with 3 features of pT, eta, phi which are transverse momentum, pseduorapidity, azimuthal angle\n",
    "        # as in Main AE paper\n",
    "        inputs = tf.reshape(inputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "        outputs = tf.reshape(outputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "\n",
    "        # extract pt\n",
    "        outputs_pt = outputs[:, :, 0]\n",
    "        \n",
    "        # extract class\n",
    "        outputs_class = outputs[:, :, 3]\n",
    "\n",
    "        # trick with phi (rescaled tanh activation function) - pi times tanh of azimuthal angle\n",
    "        outputs_phi = phi_max * tf.math.tanh(outputs[:, :, 2])\n",
    "\n",
    "        #Extracts missing transverse energy pseudorapidity outputs\n",
    "        outputs_eta_met = outputs[:, 0:1, 1]\n",
    "\n",
    "        # trick with eta (rescaled tanh activation function) - max electron pseudorapidity times tanh of pseudorapidity\n",
    "        outputs_eta_ele = ele_eta_max * tf.math.tanh(\n",
    "            outputs[:, ele_off : ele_off + nele, 1]\n",
    "        )\n",
    "\n",
    "        #Treatment of muon pseudorapidity analogous to that of electron\n",
    "        outputs_eta_mu = mu_eta_max * tf.math.tanh(outputs[:, mu_off : mu_off + nmu, 1])\n",
    "\n",
    "        #Treatment of jet pseudorapidity analogous to that of electrons and muons\n",
    "        outputs_eta_jet = jet_eta_max * tf.math.tanh(\n",
    "            outputs[:, jet_off : jet_off + njet, 1]\n",
    "        )\n",
    "\n",
    "        #Output psuedorapidity is triple with missing transverse eneergy, electron, muon, jet\n",
    "        outputs_eta = tf.concat(\n",
    "            [outputs_eta_met, outputs_eta_ele, outputs_eta_mu, outputs_eta_jet], axis=1\n",
    "        )\n",
    "\n",
    "        # use both tricks - stacks into standard triple - transvere momenta, pseudorapidity, azimuthal angle\n",
    "        outputs = tf.stack([outputs_pt, outputs_eta, outputs_phi, outputs_class], axis=-1)\n",
    "\n",
    "        # mask zero features - Zero Padding after output formation\n",
    "        mask = tf.math.not_equal(inputs, 0)\n",
    "        mask = tf.cast(mask, dtype=outputs.dtype)\n",
    "        outputs = mask * outputs\n",
    "\n",
    "        #Apply previously defined MSE_loss function 1 - corresponding to nmet\n",
    "        loss = mse_loss(\n",
    "            tf.reshape(inputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "            tf.reshape(outputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def make_mse_ae(inputs, outputs):\n",
    "    loss = make_mse_per_sample_ae_class(inputs, outputs)\n",
    "\n",
    "    loss = tf.math.reduce_mean(loss, axis=0)  # average over batch\n",
    "    loss = tf.cast(loss, dtype=inputs.dtype)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36882c-3aed-4d95-b3a9-c3bc2a605db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_ae(X, X_scaled):\n",
    "    return np.array(make_mse_per_sample_ae_class(X_scaled, model.predict(X, batch_size=1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcba64b-ac4d-46dd-9d9b-075b30ddbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled, _ = scale_pt(X_val, pt_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98873c-9820-453a-9793-147c88deaf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train[:, 1:].shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ccb7a-a9bd-4f67-a402-cafd1c3cba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, TensorBoard\n",
    "import datetime\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        min_delta=0.0001,\n",
    "        cooldown=2,\n",
    "        min_lr=1e-6,\n",
    "    ),\n",
    "    TerminateOnNaN(),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\", verbose=1, patience=10, restore_best_weights=True\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=(\"./VAELOGS\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    ),\n",
    "]\n",
    "\n",
    "inputs = Input(shape=(X_train[:, 1:].shape[1],))\n",
    "\n",
    "x = BatchNormalization()(inputs)\n",
    "x = Dense(32, kernel_initializer=HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(3, kernel_initializer=HeUniform())(x)\n",
    "x = Dense(16, kernel_initializer=HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "outputs = Dense(X_train[:, 1:].shape[1], kernel_initializer=HeUniform())(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss=make_mse_ae)\n",
    "model.fit(\n",
    "    X_train[:, 1:],\n",
    "    X_train_scaled[:, 1:],\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_val[:, 1:], X_val_scaled[:, 1:]),\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaf381-2678-4251-b9bb-cd706fef9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_loss = get_loss_ae(X_test[:, 1:], X_test_scaled[:, 1:])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "signal_data = signal_data.reshape(signal_data.shape[0], -1)\n",
    "signal_data_scaled, _ = scale_pt(signal_data, pt_scaler)\n",
    "merged_labels = np.concatenate(\n",
    "    [np.zeros(X_test.shape[0]), np.ones(signal_data.shape[0])], axis=0\n",
    ")\n",
    "signal_loss = get_loss_ae(signal_data[:, 1:], signal_data_scaled[:, 1:])\n",
    "merged_loss = np.concatenate([background_loss, signal_loss], axis=0)\n",
    "fpr, tpr, thresholds = roc_curve(merged_labels, merged_loss)\n",
    "\n",
    "tpr_1em5 = find_nearest(fpr, 1e-5)\n",
    "\n",
    "fpr_ae = fpr\n",
    "tpr_ae = tpr\n",
    "tpr_1em5_ae = tpr_1em5\n",
    "auc_ae = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59964d0f-c059-47b2-9bff-9d2363d1ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mse_per_sample_vae_class(inputs, outputs, latent_dimension):\n",
    "    mainOutputs = tf.cast(outputs[:, latent_dimension:-latent_dimension], dtype=inputs.dtype)  # make inputs and outputs same type\n",
    "    meanLatentSpaceVector = outputs[:, :latent_dimension]\n",
    "    logVarVector = outputs[:, -latent_dimension:]\n",
    "    beta = 0.5\n",
    "    klDivCoef = (beta) * -0.5\n",
    "    \n",
    "    #1+4+4+10 = 19 with 3 features of pT, eta, phi which are transverse momentum, pseduorapidity, azimuthal angle\n",
    "    # as in Main AE paper\n",
    "    inputs = tf.reshape(inputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "    mainOutputs = tf.reshape(mainOutputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "\n",
    "    # extract pt\n",
    "    outputs_pt = mainOutputs[:, :, 0]\n",
    "    \n",
    "    # extract class\n",
    "    outputs_class = mainOutputs[:, :, 3]\n",
    "    \n",
    "    # trick with phi (rescaled tanh activation function) - pi times tanh of azimuthal angle\n",
    "    outputs_phi = phi_max * tf.math.tanh(mainOutputs[:, :, 2])\n",
    "\n",
    "    #Extracts missing transverse energy pseudorapidity outputs\n",
    "    outputs_eta_met = mainOutputs[:, 0:1, 1]\n",
    "\n",
    "    # trick with eta (rescaled tanh activation function) - max electron pseudorapidity times tanh of pseudorapidity\n",
    "    outputs_eta_ele = ele_eta_max * tf.math.tanh(\n",
    "        mainOutputs[:, ele_off : ele_off + nele, 1]\n",
    "    )\n",
    "\n",
    "    #Treatment of muon pseudorapidity analogous to that of electron\n",
    "    outputs_eta_mu = mu_eta_max * tf.math.tanh(mainOutputs[:, mu_off : mu_off + nmu, 1])\n",
    "\n",
    "    #Treatment of jet pseudorapidity analogous to that of electrons and muons\n",
    "    outputs_eta_jet = jet_eta_max * tf.math.tanh(\n",
    "        mainOutputs[:, jet_off : jet_off + njet, 1]\n",
    "    )\n",
    "\n",
    "    #Output psuedorapidity is triple with missing transverse eneergy, electron, muon, jet\n",
    "    outputs_eta = tf.concat(\n",
    "        [outputs_eta_met, outputs_eta_ele, outputs_eta_mu, outputs_eta_jet], axis=1\n",
    "    )\n",
    "\n",
    "    # use both tricks - stacks into standard triple - transvere momenta, pseudorapidity, azimuthal angle\n",
    "    mainOutputs = tf.stack([outputs_pt, outputs_eta, outputs_phi, outputs_class], axis=-1)\n",
    "\n",
    "    # mask zero features - Zero Padding after output formation\n",
    "    mask = tf.math.not_equal(inputs, 0)\n",
    "    mask = tf.cast(mask, dtype=mainOutputs.dtype)\n",
    "    mainOutputs = mask * mainOutputs\n",
    "\n",
    "    #Apply previously defined MSE_loss function 1 - corresponding to nmet\n",
    "    mse_loss_value = mse_loss(\n",
    "        tf.reshape(inputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "        tf.reshape(mainOutputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "    )\n",
    "    \n",
    "    mse_loss_value = tf.math.reduce_mean(mse_loss_value, axis=0)\n",
    "    \n",
    "    kl_divergence = tf.math.multiply(klDivCoef, tf.reduce_sum(1 + logVarVector - tf.square(meanLatentSpaceVector) - tf.exp(logVarVector), axis=-1))\n",
    "    kl_divergence = tf.math.reduce_mean(kl_divergence, axis=0)\n",
    "    kl_divergence = tf.cast(kl_divergence, dtype = mse_loss_value.dtype)\n",
    "\n",
    "    # Apply previously defined MSE_loss function 1 - corresponding to nmet\n",
    "    loss = tf.math.add(tf.math.multiply((1 - beta), mse_loss_value), kl_divergence)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def make_mse_vae(inputs, outputs):\n",
    "    loss = make_mse_per_sample_vae_class(inputs, outputs, latent_dim)\n",
    "\n",
    "    #loss = tf.math.reduce_mean(loss, axis=0)  # average over batch\n",
    "    loss = tf.cast(loss, dtype=inputs.dtype)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def mod_make_mse_per_sample_vae_class(inputs, outputs, latent_dimension):\n",
    "    mainOutputs = tf.cast(outputs[:, latent_dimension:-latent_dimension], dtype=inputs.dtype)  # make inputs and outputs same type\n",
    "    meanLatentSpaceVector = outputs[:, :latent_dimension]\n",
    "    logVarVector = outputs[:, -latent_dimension:]\n",
    "    beta = 0.5\n",
    "    klDivCoef = (beta) * -0.5\n",
    "    \n",
    "    #1+4+4+10 = 19 with 3 features of pT, eta, phi which are transverse momentum, pseduorapidity, azimuthal angle\n",
    "    # as in Main AE paper\n",
    "    inputs = tf.reshape(inputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "    mainOutputs = tf.reshape(mainOutputs, [-1, (nmet + nele + nmu + njet), nfeat])\n",
    "\n",
    "    # extract pt\n",
    "    outputs_pt = mainOutputs[:, :, 0]\n",
    "    \n",
    "    # extract class\n",
    "    outputs_class = mainOutputs[:, :, 3]\n",
    "    \n",
    "    # trick with phi (rescaled tanh activation function) - pi times tanh of azimuthal angle\n",
    "    outputs_phi = phi_max * tf.math.tanh(mainOutputs[:, :, 2])\n",
    "\n",
    "    #Extracts missing transverse energy pseudorapidity outputs\n",
    "    outputs_eta_met = mainOutputs[:, 0:1, 1]\n",
    "\n",
    "    # trick with eta (rescaled tanh activation function) - max electron pseudorapidity times tanh of pseudorapidity\n",
    "    outputs_eta_ele = ele_eta_max * tf.math.tanh(\n",
    "        mainOutputs[:, ele_off : ele_off + nele, 1]\n",
    "    )\n",
    "\n",
    "    #Treatment of muon pseudorapidity analogous to that of electron\n",
    "    outputs_eta_mu = mu_eta_max * tf.math.tanh(mainOutputs[:, mu_off : mu_off + nmu, 1])\n",
    "\n",
    "    #Treatment of jet pseudorapidity analogous to that of electrons and muons\n",
    "    outputs_eta_jet = jet_eta_max * tf.math.tanh(\n",
    "        mainOutputs[:, jet_off : jet_off + njet, 1]\n",
    "    )\n",
    "\n",
    "    #Output psuedorapidity is triple with missing transverse eneergy, electron, muon, jet\n",
    "    outputs_eta = tf.concat(\n",
    "        [outputs_eta_met, outputs_eta_ele, outputs_eta_mu, outputs_eta_jet], axis=1\n",
    "    )\n",
    "\n",
    "    # use both tricks - stacks into standard triple - transvere momenta, pseudorapidity, azimuthal angle\n",
    "    mainOutputs = tf.stack([outputs_pt, outputs_eta, outputs_phi, outputs_class], axis=-1)\n",
    "\n",
    "    # mask zero features - Zero Padding after output formation\n",
    "    mask = tf.math.not_equal(inputs, 0)\n",
    "    mask = tf.cast(mask, dtype=mainOutputs.dtype)\n",
    "    mainOutputs = mask * mainOutputs\n",
    "\n",
    "    #Apply previously defined MSE_loss function 1 - corresponding to nmet\n",
    "    mse_loss_value = mse_loss(\n",
    "        tf.reshape(inputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "        tf.reshape(mainOutputs, [-1, (1 + nele + nmu + njet) * nfeat]),\n",
    "    )\n",
    "    \n",
    "    #mse_loss_value = tf.math.reduce_mean(mse_loss_value, axis=0)\n",
    "    \n",
    "    kl_divergence = tf.math.multiply(klDivCoef, tf.reduce_sum(1 + logVarVector - tf.square(meanLatentSpaceVector) - tf.exp(logVarVector), axis=-1))\n",
    "    #kl_divergence = tf.math.reduce_mean(kl_divergence, axis=0)\n",
    "    kl_divergence = tf.cast(kl_divergence, dtype = mse_loss_value.dtype)\n",
    "\n",
    "    # Apply previously defined MSE_loss function 1 - corresponding to nmet\n",
    "    loss = tf.math.add(tf.math.multiply((1 - beta), mse_loss_value), kl_divergence)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb6bda-2cd6-467c-a5bb-34f7687cf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_vae(X, X_scaled):\n",
    "    return np.array(mod_make_mse_per_sample_vae_class(X_scaled, model.predict(X, batch_size=1024), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab62522-0130-4357-bd03-877cfe3f1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac345f-e0dd-42a2-a52e-15a70912801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Concatenate\n",
    "\n",
    "# Custom sampling layer to generate epsilon and compute z\n",
    "def sampling(args):\n",
    "    mean, log_var = args\n",
    "    epsilon = tf.random.normal(tf.shape(mean), mean=0.0, stddev=1.0)\n",
    "    return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "    \n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        min_delta=0.0001,\n",
    "        cooldown=2,\n",
    "        min_lr=1e-6,\n",
    "    ),\n",
    "    TerminateOnNaN(),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\", verbose=1, patience=10, restore_best_weights=True\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=(\"./VAELOGS\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    ),\n",
    "]\n",
    "\n",
    "inputs = Input(shape=(X_train[:, 1:].shape[1],))\n",
    "\n",
    "x = BatchNormalization()(inputs)\n",
    "#Block 1\n",
    "x = Dense(32, kernel_initializer=HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "\n",
    "#Block 2\n",
    "x = Dense(16, kernel_initializer=HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "\n",
    "#Block 3\n",
    "x = Dense(latent_dim, kernel_initializer=HeUniform())(x)\n",
    "meanLatentSpaceVector = Dense(3, activation='linear')(x)\n",
    "logVarVector = Dense(3, activation='linear')(x)\n",
    "# epsilon = tf.random.normal(tf.shape(meanLatentSpaceVector), mean=0.0, stddev=1.0)\n",
    "# z = meanLatentSpaceVector + tf.exp(0.5 * logVarVector) * epsilon\n",
    "z = Lambda(sampling)([meanLatentSpaceVector, logVarVector])\n",
    "\n",
    "intermediate = z\n",
    "\n",
    "# Block 4\n",
    "z = Dense(16, kernel_initializer=HeUniform())(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = LeakyReLU(alpha=0.3)(z)\n",
    "\n",
    "# Block 5\n",
    "z = Dense(32, kernel_initializer=HeUniform())(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = LeakyReLU(alpha=0.3)(z)\n",
    "\n",
    "# decoderEpsilon = tf.random.normal(tf.shape(meanLatentSpaceVector), mean=0.0, stddev=1.0)\n",
    "# decoderZ = meanLatentSpaceVector + tf.exp(0.5 * logVarVector) * decoderEpsilon\n",
    "\n",
    "# Output Layer\n",
    "outputs = Dense(X_train[:, 1:].shape[1], kernel_initializer=HeUniform())(z)\n",
    "outputs = Concatenate(axis=1)([meanLatentSpaceVector, outputs, logVarVector])\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss=make_mse_vae)\n",
    "model.fit(\n",
    "    X_train[:, 1:],\n",
    "    X_train_scaled[:, 1:],\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_val[:, 1:], X_val_scaled[:, 1:]),\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744dc0c-372c-470d-bb32-b60873588bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_loss = get_loss_vae(X_test[:, 1:], X_test_scaled[:, 1:])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "signal_data = signal_data.reshape(signal_data.shape[0], -1)\n",
    "signal_data_scaled, _ = scale_pt(signal_data, pt_scaler)\n",
    "merged_labels = np.concatenate(\n",
    "    [np.zeros(X_test.shape[0]), np.ones(signal_data.shape[0])], axis=0\n",
    ")\n",
    "signal_loss = get_loss_vae(signal_data[:, 1:], signal_data_scaled[:, 1:])\n",
    "merged_loss = np.concatenate([background_loss, signal_loss], axis=0)\n",
    "fpr, tpr, thresholds = roc_curve(merged_labels, merged_loss)\n",
    "\n",
    "tpr_1em5 = find_nearest(fpr, 1e-5)\n",
    "\n",
    "fpr_vae = fpr\n",
    "tpr_vae = tpr\n",
    "tpr_1em5_vae = tpr_1em5\n",
    "auc_vae = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba15f76-48b9-484e-aea0-07d5ab76cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_gaussian,\n",
    "         tpr_gaussian,\n",
    "         label=f\"Gaussian, AUC={auc(fpr_gaussian, tpr_gaussian)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_gaussian[tpr_1em5_gaussian]*100:.3f}%\")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_pumap,\n",
    "    tpr_pumap,\n",
    "    label=f\"PUMAP, AUC={auc(fpr_pumap, tpr_pumap)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pumap[tpr_1em5_pumap]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_pca,\n",
    "    tpr_pca,\n",
    "    label=f\"PCA-10, AUC={auc(fpr_pca, tpr_pca)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca[tpr_1em5_pca]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_ae,\n",
    "    tpr_ae,\n",
    "    label=f\"DNNAE, AUC={auc(fpr_ae, tpr_ae)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_ae[tpr_1em5_ae]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_vae,\n",
    "    tpr_vae,\n",
    "    label=f\"DNNVAE, AUC={auc(fpr_vae, tpr_vae)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_vae[tpr_1em5_vae]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.legend(title=f\"{signal_label} Baseline\")\n",
    "plt.plot([1e-6, 1], [1e-6, 1], \"k--\")\n",
    "plt.plot([1e-5, 1e-5], [1e-6, 1], \"r-.\")\n",
    "plt.xlim([1e-6, 1])\n",
    "plt.ylim([1e-6, 1])\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68764c35-43a9-47e9-87f3-8334988706b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc as auc\n",
    "def PCA_ROC(signal_data, n_comp):\n",
    "    model = PCA(n_components=n_comp)\n",
    "    \n",
    "    trainEmbedding = model.fit_transform(X_train_scaled)\n",
    "    inv_transform_training_data = model.inverse_transform(trainEmbedding)\n",
    "    background_loss = get_loss_pca(X_train_scaled[:, 1:], inv_transform_training_data[:, 1:])\n",
    "    \n",
    "    # _ = plt.figure()\n",
    "    signal_data = signal_data.reshape(signal_data.shape[0], -1)\n",
    "    signal_data_scaled, _ = scale_pt(signal_data, pt_scaler)\n",
    "    merged_labels = np.concatenate(\n",
    "        [np.zeros(X_train.shape[0]), np.ones(signal_data.shape[0])], axis=0\n",
    "    )\n",
    "    \n",
    "    # print(\"Starting PCA signal embedding: \" + signal_label)\n",
    "    inv_transform_signal_data = model.inverse_transform(model.transform(signal_data_scaled))\n",
    "    \n",
    "    signal_loss = get_loss_pca(signal_data_scaled[:, 1:], inv_transform_signal_data[:, 1:])\n",
    "    merged_loss = np.concatenate([background_loss, signal_loss], axis=0)\n",
    "    \n",
    "    # print(\"Successful PCA signal embedding, inverse transform, and loss computations: \" + signal_label)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(merged_labels, merged_loss)\n",
    "    tpr_1em5 = find_nearest(fpr, 1e-5)\n",
    "    auc_res = auc(fpr, tpr)\n",
    "    \n",
    "    return (fpr, tpr, tpr_1em5, auc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a76a75-cd3e-4fb0-8110-7b6c2cd49882",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fpr_pca_two, tpr_pca_two, tpr_1em5_pca_two, auc_pca_two) = PCA_ROC(signal_data, 2)\n",
    "(fpr_pca_three, tpr_pca_three, tpr_1em5_pca_three, auc_pca_three) = PCA_ROC(signal_data, 3)\n",
    "(fpr_pca_four, tpr_pca_four, tpr_1em5_pca_four, auc_pca_four) = PCA_ROC(signal_data, 4)\n",
    "(fpr_pca_eight, tpr_pca_eight, tpr_1em5_pca_eight, auc_pca_eight) = PCA_ROC(signal_data, 8)\n",
    "(fpr_pca_ten, tpr_pca_ten, tpr_1em5_pca_ten, auc_pca_ten) = PCA_ROC(signal_data, 10)\n",
    "(fpr_pca_twelve, tpr_pca_twelve, tpr_1em5_pca_twelve, auc_pca_twelve) = PCA_ROC(signal_data, 12)\n",
    "(fpr_pca_thirteen, tpr_pca_thirteen, tpr_1em5_pca_thirteen, auc_pca_thirteen) = PCA_ROC(signal_data, 13)\n",
    "(fpr_pca_sixteen, tpr_pca_sixteen, tpr_1em5_pca_sixteen, auc_pca_sixteen) = PCA_ROC(signal_data, 16)\n",
    "(fpr_pca_thirtytwo, tpr_pca_thirtytwo, tpr_1em5_pca_thirtytwo, auc_pca_thirtytwo) = PCA_ROC(signal_data, 32)\n",
    "(fpr_pca_sixtyfour, tpr_pca_sixtyfour, tpr_1em5_pca_sixtyfour, auc_pca_sixtyfour) = PCA_ROC(signal_data, 64)\n",
    "(fpr_pca_seventy, tpr_pca_seventy, tpr_1em5_pca_seventy, auc_pca_seventy) = PCA_ROC(signal_data, 70)\n",
    "(fpr_pca_seventysix, tpr_pca_seventysix, tpr_1em5_pca_seventysix, auc_pca_seventysix) = PCA_ROC(signal_data, 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e6e04-9555-4f4a-9862-9446c4a97381",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dict = {}\n",
    "tpr_1em5_dict = {}\n",
    "\n",
    "for i in range(2, 77):\n",
    "    (fpr_pca_test, tpr_pca_test, tpr_1em5_pca_test, auc_pca_test) = PCA_ROC(signal_data, i)\n",
    "    auc_dict[i] = auc_pca_test\n",
    "    tpr_1em5_dict[i] = tpr_1em5_pca_test\n",
    "\n",
    "sorted_dict_auc = dict(sorted(auc_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_dict_tpr_1em5 = dict(sorted(tpr_1em5_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print header\n",
    "print(f\"{'PCA Components':<15} {'AUC (%)':<10} {'TPR@FPR 10^{-5} (%)':<10}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Print each row\n",
    "for n_components, auc in sorted_dict_auc.items():\n",
    "    tpr_1em5 = sorted_dict_tpr_1em5[n_components]\n",
    "    print(f\"{n_components:<15} {auc*100:.2f}% {tpr_1em5*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9cd8d-e32d-4d03-9ff3-cfa6c82ddb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from matplotlib.legend import Legend\n",
    "from sklearn.metrics import auc as auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "def plot_PCA(fpr_pca, tpr_pca, tpr_1em5_pca, auc_pca, n_comp):\n",
    "    plt.plot(\n",
    "        fpr_pca,\n",
    "        tpr_pca,\n",
    "        label=f\"PCA-{n_comp}, AUC={auc(fpr_pca, tpr_pca)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca[tpr_1em5_pca]*100:.3f}%\",\n",
    "    )\n",
    "\n",
    "# Add your data for plotting\n",
    "plot_PCA(fpr_pca_two, tpr_pca_two, tpr_1em5_pca_two, auc_pca_two, 2)\n",
    "plot_PCA(fpr_pca_three, tpr_pca_three, tpr_1em5_pca_three, auc_pca_three, 3)\n",
    "plot_PCA(fpr_pca_four, tpr_pca_four, tpr_1em5_pca_four, auc_pca_four, 4)\n",
    "plot_PCA(fpr_pca_eight, tpr_pca_eight, tpr_1em5_pca_eight, auc_pca_eight, 8)\n",
    "plot_PCA(fpr_pca_ten, tpr_pca_ten, tpr_1em5_pca_ten, auc_pca_ten, 10)\n",
    "plot_PCA(fpr_pca_twelve, tpr_pca_twelve, tpr_1em5_pca_twelve, auc_pca_twelve, 12)\n",
    "plot_PCA(fpr_pca_thirteen, tpr_pca_thirteen, tpr_1em5_pca_thirteen, auc_pca_thirteen, 13)\n",
    "plot_PCA(fpr_pca_sixteen, tpr_pca_sixteen, tpr_1em5_pca_sixteen, auc_pca_sixteen, 16)\n",
    "plot_PCA(fpr_pca_thirtytwo, tpr_pca_thirtytwo, tpr_1em5_pca_thirtytwo, auc_pca_thirtytwo, 32)\n",
    "plot_PCA(fpr_pca_sixtyfour, tpr_pca_sixtyfour, tpr_1em5_pca_sixtyfour, auc_pca_sixtyfour, 64)\n",
    "plot_PCA(fpr_pca_seventy, tpr_pca_seventy, tpr_1em5_pca_seventy, auc_pca_seventy, 70)\n",
    "plot_PCA(fpr_pca_seventysix, tpr_pca_seventysix, tpr_1em5_pca_seventysix, auc_pca_seventysix, 76)\n",
    "\n",
    "# Add lines and set axes limits\n",
    "plt.plot([1e-6, 1], [1e-6, 1], \"k--\")\n",
    "plt.plot([1e-5, 1e-5], [1e-6, 1], \"r-.\")\n",
    "plt.xlim([1e-6, 1])\n",
    "plt.ylim([1e-6, 1])\n",
    "plt.loglog()\n",
    "\n",
    "# Add legend to the plot\n",
    "legend = plt.legend(\n",
    "    title=f\"{signal_label} Baseline\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0.0\n",
    ")\n",
    "\n",
    "# Save the main plot without the legend\n",
    "pca_roc_file_name = f\"New-PCA-ROC-Curve-B{random_background_indices.size}-S{random_signal_indices.size}\"\n",
    "plt.savefig(f\"{pca_roc_file_name}.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create a separate figure for the legend\n",
    "# fig, ax = plt.subplots(figsize=(4, 4))  # Adjust the figure size\n",
    "# legend_fig = ax.legend_ = legend  # Use the existing legend\n",
    "# ax.axis(\"off\")  # Turn off axes for the legend figure\n",
    "\n",
    "# # Save the legend as a separate image\n",
    "# legend_file_name = f\"{roc_file_name}_legend.png\"\n",
    "# fig.savefig(legend_file_name, format='png', dpi=300, bbox_inches='tight')\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad5193-e1ca-4140-88a3-d5a2fd23356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = max(sorted_dict_auc, key=sorted_dict_auc.get)\n",
    "k_prime = max(sorted_dict_tpr_1em5, key=sorted_dict_tpr_1em5.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6bdf9-6959-43a2-bf93-68e01248f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k)\n",
    "print(k_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2987-6ea4-4898-9878-d8f00a5342d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get PCA-2, PCA-3, PCA-4, PCA with max AUC, PCA with max TPR@FPR 10^-5\n",
    "\n",
    "(fpr_pca_target, tpr_pca_target, tpr_1em5_pca_target, auc_pca_target) = PCA_ROC(signal_data, k)\n",
    "(fpr_pca_target_two, tpr_pca_target_two, tpr_1em5_pca_target_two, auc_pca_target_two) = PCA_ROC(signal_data, k_prime)\n",
    "\n",
    "plt.plot(fpr_gaussian,\n",
    "         tpr_gaussian,\n",
    "         label=f\"Gaussian, AUC={auc(fpr_gaussian, tpr_gaussian)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_gaussian[tpr_1em5_gaussian]*100:.3f}%\")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_pumap,\n",
    "    tpr_pumap,\n",
    "    label=f\"PUMAP, AUC={auc(fpr_pumap, tpr_pumap)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pumap[tpr_1em5_pumap]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "#PCA-2\n",
    "\n",
    "plt.plot(\n",
    "    fpr_pca_two,\n",
    "    tpr_pca_two,\n",
    "    label=f\"PCA-2, AUC={auc(fpr_pca_two, tpr_pca_two)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca_two[tpr_1em5_pca_two]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "#PCA-3\n",
    "\n",
    "plt.plot(\n",
    "    fpr_pca_three,\n",
    "    tpr_pca_three,\n",
    "    label=f\"PCA-3, AUC={auc(fpr_pca_three, tpr_pca_three)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca_three[tpr_1em5_pca_three]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "\n",
    "#PCA-4\n",
    "\n",
    "plt.plot(\n",
    "    fpr_pca_four,\n",
    "    tpr_pca_four,\n",
    "    label=f\"PCA-4, AUC={auc(fpr_pca_four, tpr_pca_four)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca_four[tpr_1em5_pca_four]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "\n",
    "#PCA with max AUC\n",
    "plt.plot(\n",
    "    fpr_pca_target,\n",
    "    tpr_pca_target,\n",
    "    label=f\"PCA-{k}, AUC={auc(fpr_pca_target, tpr_pca_target)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca_target[tpr_1em5_pca_target]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "#PCA with max TPR@FPR 10^-5\n",
    "plt.plot(\n",
    "    fpr_pca_target_two,\n",
    "    tpr_pca_target_two,\n",
    "    label=f\"PCA-{k_prime}, AUC={auc(fpr_pca_target_two, tpr_pca_target_two)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_pca_target_two[tpr_1em5_pca_target_two]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_ae,\n",
    "    tpr_ae,\n",
    "    label=f\"DNNAE, AUC={auc(fpr_ae, tpr_ae)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_ae[tpr_1em5_ae]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr_vae,\n",
    "    tpr_vae,\n",
    "    label=f\"DNNVAE, AUC={auc(fpr_vae, tpr_vae)*100:.2f}%, TPR@FPR $10^{{-5}}$={tpr_vae[tpr_1em5_vae]*100:.3f}%\",\n",
    ")\n",
    "\n",
    "plt.legend(title=f\"{signal_label} Baseline\")\n",
    "plt.plot([1e-6, 1], [1e-6, 1], \"k--\")\n",
    "plt.plot([1e-5, 1e-5], [1e-6, 1], \"r-.\")\n",
    "plt.xlim([1e-6, 1])\n",
    "plt.ylim([1e-6, 1])\n",
    "plt.loglog()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "roc_file_name = f\"New-ROC-Curve-B{random_background_indices.size}-S{random_signal_indices.size}\"\n",
    "plt.savefig(f\"{roc_file_name}.png\", format='png', dpi=300)  # High resolution\n",
    "plt.close()  # Close the figure to release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdafbc5-4bee-4b41-859a-028ba212b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "from pathlib import Path\n",
    "\n",
    "# Email details\n",
    "sender_email = \"rosachdeva@ucsd.edu\"\n",
    "receiver_email = \"rosachdeva@ucsd.edu\"\n",
    "password = \"uuvo esud bmib fvvk\"  # Use app-specific password if using Gmail\n",
    "\n",
    "# Create the email message\n",
    "msg = EmailMessage()\n",
    "msg['Subject'] = 'Final ROC Curve Results'\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = receiver_email\n",
    "\n",
    "# Attach text content\n",
    "msg.set_content(\"Please find the final ROC curve attached.\")\n",
    "\n",
    "# Add the ROC curve image\n",
    "roc_curve_path = './' + roc_file_name + \".png\"  # Path to the ROC curve image\n",
    "pca_roc_curve_path = './' + pca_roc_file_name + \".png\"  # Path to the PCA ROC curve image\n",
    "if Path(roc_curve_path).exists():\n",
    "    with open(roc_curve_path, 'rb') as img:\n",
    "        img_data = img.read()\n",
    "        msg.add_attachment(img_data, maintype='image', subtype='png', filename=\"roc_curve.png\")\n",
    "else:\n",
    "    print(f\"Error: {roc_curve_path} not found. Ensure the ROC curve image is saved.\")\n",
    "\n",
    "if Path(pca_roc_curve_path).exists():\n",
    "    with open(pca_roc_curve_path, 'rb') as img:\n",
    "        img_data = img.read()\n",
    "        msg.add_attachment(img_data, maintype='image', subtype='png', filename=\"pca_roc_curve.png\")\n",
    "else:\n",
    "    print(f\"Error: {roc_curve_path} not found. Ensure the ROC curve image is saved.\")\n",
    "\n",
    "\n",
    "\n",
    "# Send the email\n",
    "try:\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:  # For Gmail. Use other SMTP servers if needed.\n",
    "        server.login(sender_email, password)\n",
    "        server.send_message(msg)\n",
    "    print(\"Email sent successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending email: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fea29-914c-4c6e-9b74-acf3f589da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the requirements.txt\n",
    "try:\n",
    "    with open(\"requirements.txt\", \"w\") as f:\n",
    "        subprocess.run([\"pip\", \"freeze\"], stdout=f, check=True)\n",
    "    print(\"requirements.txt saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
